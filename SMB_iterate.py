import gym_super_mario_bros
import nes_py
import pandas as pd
import numpy as np
from time import monotonic
import os
import matplotlib.pyplot as plt
from collections import deque
from PIL import Image, ImageDraw, ImageFont
import copy as copycopy # for deep copying? see: 
# https://numpy.org/doc/stable/reference/generated/numpy.ndarray.copy.html#numpy.ndarray.copy

from itertools import combinations # combinations for a complete actionSpace


from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY
from random import seed as randomSeed
from random import shuffle


# see action_space_and_TAS.ipynb
ALL_BUTTONS = ['right', 'left', 'down', 'up', 'start', 'select', 'B', 'A']
ALL_SINGLE_ACTIONS = ALL_BUTTONS.copy()
ALL_SINGLE_ACTIONS.append("NOOP")
COMPLETE_ACTIONSPACE = [ ["NOOP"] ] # note use of a list of lists 
for i in range(1, len(ALL_BUTTONS) + 1): # do not include the empty permutation
                                         # already included above
    for j in combinations(ALL_BUTTONS, i):
        COMPLETE_ACTIONSPACE.append(list(list(j)))


# does not seem happy to work in a tensorflow kernel
def save_diagnostic_image(frameArray, step, action, x_pos, y_pos):
    """ Save a snapshot with additional text info burned in."""
    image = Image.fromarray(frameArray.copy())
    
    draw = ImageDraw.Draw(image)
    ## fails in TensorFlow environment
    #font = ImageFont.truetype("arial.ttf", size = 20)
    text_annotation = ""
    text_annotation += str(f"step: {step:0>6}\naction: {action}\n")
    text_annotation += str(f"x: {x_pos:0>3}, y: {y_pos:0>3}\n")

    #white_text = (255, 255, 255)
    black_text = (0,   0,   0  )
    draw.text((0, 0), text_annotation, fill = black_text)

    # use of padding in filename is helpful for passing 
    # in to Kdenlive as an Image Sequence for video review
    # in quick testing, .png was actually smaller than .jpeg
    image.save(f"./states/{step:0>6}_{monotonic()}.png")
    

def initialize_environment(mode, rom):
    """Initialize environment in gymnasium. 
    Sets the list of acceptable actions."""


    if rom == 'v0':
        # v0 is a more traditional view
        # v0 corresponds to 'super-mario-bros.nes' included with package
        # with MD5 of: 673913a23cd612daf5ad32d4085e0760
        # corresponding to "Super Mario Bros. (E) (REVA) [!].nes SourceDB: GoodNES 3.23"
        # as per: https://tasvideos.org/Games/1/Versions/List
        env = gym_super_mario_bros.make('SuperMarioBros-v0')
    elif rom == 'v3':
        # v3 is a simplified rectangular view
        # this may have been generated by kautenja
        # does not appear in the TAS collections that I saw
        env = gym_super_mario_bros.make('SuperMarioBros-v3') # a warning message recommends v3
    
    if mode == "simple":
        env = JoypadSpace(env, SIMPLE_MOVEMENT)
        actionSpace = SIMPLE_MOVEMENT
    elif mode == "complex":
        env = JoypadSpace(env, COMPLEX_MOVEMENT)
        actionSpace = COMPLEX_MOVEMENT
    elif mode == "rightOnly":
        env = JoypadSpace(env, RIGHT_ONLY)
        actionSpace = RIGHT_ONLY
    else:
        # provide a predefined list of string actions
        env = JoypadSpace(env, mode)
        actionSpace = mode

    #print(f"Initializing environment: {repr(actionSpace)=}")
    return( (env, actionSpace) )

class Agent():
    
    def __init__(self, actionSpace, rom = 'v0', seed = 5004):

        # tracking helpful measures for diagnostics/debugging
        self.x_positions = []
        self.y_positions = []
        self.actions = []
        self.timePoints = [(monotonic(), 0)] # collect the start point, and append any new lives

        self.step = -1
        self.cumulativeReward = 0
        self.terminated = False
        self.truncated = False
        self.prior_time = None
        self.state = None
        self.seed = seed

        # a number of states to collect for manual review
        queueLength = 500
        self.trailingStates = deque() # double-ended queue
        
        # pre-populate to avoid edge cases, like popping from empty
        for i in range(0, queueLength):
            self.trailingStates.append(None)
        
        self.env, self.actionSpace = initialize_environment(actionSpace, rom)

        # check for whether seed is specified?
        # this unfortunately is presently inadequate to guarantee stability
        state = self.env.reset(seed = self.seed)
        np.random.seed(self.seed)
        randomSeed(self.seed)        

    def __repr__(self):
        result = ""
        length = len(self.actionSpace)
        if length <= 18: #18 is the length of the Happy Lee TAS actionSpace
            result += f"{self.actionSpace=}\n"
        else:
            result += f"actionSpace is long, (length: {length}), showing first 5 and last 5 entries:\n"
            result += f"{str(self.actionSpace[0:5])}\n ... \n {str(self.actionSpace[-5:])}\n"
        result += f"{self.seed=}\n"
        result += f"{self.step=}\n"
        result += f"{self.cumulativeReward=}\n"
        result += f"Latest state:\n{self.state}"

        return(result)
    
    def iterate(self, policy, maxSteps = None, saveImage = False):
        if isinstance(policy, list):
            sequence = True
            policyIndex = 0

            # don't go beyond policy
            # may be interesting to switch from rigid sequence to policy
            # partway through an episode, but not currently in scope
            maxSteps = len(policy)

        
        while True:
            self.step += 1

            # break early
            if maxSteps is not None:
                if self.step >= maxSteps:
                    break

            if policy == "random":
                # env.action_space is discrete, a range over the length of the number of actions
                # may be simpler to specify otherwise to avoid confusion with actionSpace
                action = self.env.action_space.sample() # as integer index
                action_text = self.actionSpace[action] # for diagnostic
            elif sequence:
                # could do with renaming, but at this point the policy *is* a list of inputs
                action_text = policy[policyIndex]
                action = self.actionSpace.index(action_text)
                #action = self.env.action_space.index(action_text)
                policyIndex += 1
            
            self.state, reward, terminated, truncated, info = self.env.step(action)
        
            if terminated or truncated:
                print(f"{terminated=}\n{truncated=}")
                print(f"{info=}")
                break
        
            # diagnostic info collection
            self.cumulativeReward += reward
            self.x_positions.append(info['x_pos'])
            self.y_positions.append(info['y_pos'])
            self.actions.append(action_text)
            # remove first, add to end of deque
            self.trailingStates.popleft()
            self.trailingStates.append(self.state.copy()) # copy it over!
        
            # diagnostic info printing
            # print periodically, and as mario is timing out
            if self.step % 1_000 == 0 or info['time'] <= 0 :
                print(f"{self.step=:0>7}, {self.cumulativeReward=}, {info['coins']=}, {info['time']=}")
            
            # collect information regarding end of life, time points
            if self.prior_time is not None:
                if self.prior_time < info['time']:
                    print("start of new life")
                    print("current lives: ", info['life'])
                    self.timePoints.append( (monotonic(), self.step) )
                    break # just do one life
                self.prior_time = info['time']
            if self.prior_time is None:
                self.prior_time = info['time']

            if saveImage:
                save_diagnostic_image(self.state, self.step, action_text, info['x_pos'], info['y_pos'])
        
        self.env.close()

    def actionDF(self):
        """A simple dataframe overview of actions taken."""
        
        df = pd.DataFrame(self.actions)

        # variable numbers of possible simultaneous actions
        for index, col in enumerate(df.columns):
            df = df.rename({
                index : 'a' + str(index)
            }, axis = 1)
        
        df = df.fillna('') # None to ''
        df['action'] = ''
        
        for col in df.columns:
            if col[0] == 'a' and col != 'action':
                df['action'] = df['action'] + ',' + df[col]
                
        df['action'] = df['action'].str.strip(',')
        df['buttonCount'] = df['action'].str.count(',') + 1
        df['buttonCount'] = df.apply(lambda x : 0 if x['action'] == "NOOP" else x['buttonCount'], axis = 1)
        df = df.astype({'buttonCount' : 'int32'})
        
        return(df)