# training/main loop skeleton?

"""
Initialize replay memory D to capacity N
Initialize action-value function Q with random weights 

for episode = 1, M do 
    Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)
    for t = 1, T do  
        With probability ϵ select a random action a_t
        otherwise select a_t = max_a Q^*(φ(st), a; θ) 
        Execute action a_t in emulator and observe reward r_t and image x_{t+1}
        Set s_{t+1} = s_t, a_t, x_{t+1} and preprocess φ_{t+1} = φ(s_{t+1})
        Store transition (φ_t, a_t, rt, φt+1) in D 
        Sample random minibatch of transitions (φ_j, a_j, r_j, φ_{j+1}) from D  
        Set y_j =  { r_j for terminal φ_{j+1}
                     r_j + γ max_{a′} Q(φ_{j+1}, a′; θ) for non-terminal φ_{j+1} 
        Perform a gradient descent step on (yj − Q(φj, aj; θ))2 according to equation 3
    end for 
end for

"""
# ------------------------------------
# libraries

# figures/processing
import pandas as pd
import numpy as np

# logging
import csv

# environment
import gym_super_mario_bros 
import nes_py      
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY

# saving outputs
# from time import monotonic  
import os
#from pathlib import Path # https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories
import datetime
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont

from random import randint, random

import sys #argument handling

#from collections import deque
import torch
import torch.optim as optim
import torchvision # grayscale

import DQN
from replay_buffer import ReplayBuffer
from DQN import DQN
from itertools import combinations # combinations for a complete actionSpace

# ------------------------------------
# functions
def prefillBuffer(BUFFER_SIZE, env, actionSpace):
    """ prefill the replay buffer

    Note that if random levels are selected outside this function, it is likely that the 
    buffer size may need to be much larger than if one focused on just one level.
    """

    # grayscale
    rb = ReplayBuffer(BUFFER_SIZE, (1, ADJ_FRAME_HEIGHT, ADJ_FRAME_WIDTH) )

    # need an environment to get transitions
    state, info = env.reset(seed = seed) if SEED else env.reset()

    # uniform random action selection
    # it may be worth pre-weighting here with TAS inputs
    # see 'on TAS and ROMs.ipynb'
    # Space.sample(probability) as per: https://gymnasium.farama.org/api/spaces/
    state, reward, terminated, truncated, info = env.step(env.action_space.sample())


    resetCount              = 0
    transitionCount         = 0 # counter of transitions filled into buffer
    transitionsCurrentLevel = 0 # counter of per-level transitions, 
                                # helps to avoid having too many transitions from one level
    while transitionCount < BUFFER_SIZE:
        
        old_state = state
        state, reward, terminated, truncated, info = env.step(env.action_space.sample())

        # reset environment if dead, or too many samples from current environment
        if terminated or truncated or transitionsCurrentLevel % (BUFFER_SIZE // 10) == 0:
            state, info = env.reset(seed = seed) if SEED else env.reset()
            state, reward, terminated, truncated, info = env.step(env.action_space.sample())
            resetCount += 1
            transitionsCurrentLevel = 0
            next
            
        rb.storeTransition((
            preprocessFrame(old_state),
            torch.randint(low = 0, high = env.action_space.n, size = (1,)),
            torch.randint(high = 10, size = (1,)),   
            preprocessFrame(state)
        ))

        transitionCount += 1
        transitionsCurrentLevel += 1

        if BUFFER_SIZE > 10:
            if i % (BUFFER_SIZE // 10) == 0:
                print(f"Filling buffer slot {i} of {BUFFER_SIZE}")
    print(f"Filling buffer sampled from {resetCount} level starts (not guaranteed unique)") 

    return(rb)

def preprocessFrame(frame):
    """ Preprocess an input image frame

    Inputs:
        frame - frame of RGB image data
        blur - boolean for whether to apply blur
        vTrim - boolean for whether to apply vertical trimming
        vTrimOffset - the amount of vertical trimming in pixels from the top

    Note that the stacking of sequential frames, as per Mnih et al. 2013 
    is left to external logic
    """
    
    # this seems...like a major processing step that will slow training
    # might be interesting to benchmark this

    # note this is rows, columns, channels
    # grayscale
    frame = frame[VTRIM:, :, :]
    frame = frame[:, HTRIM:FRAME_WIDTH - HTRIM_RIGHT, :]

    # this block was generated by generative AI, copilot
    # Convert NumPy array to torch tensor
    if isinstance(frame, np.ndarray):
        frame = frame.copy()  # Ensure positive strides 
        frame = torch.from_numpy(frame).float()  # Convert to float tensor
        frame = frame.permute(2, 0, 1)  # Change shape from (H, W, C) to (C, H, W)
    # end generative AI block
    
    #grayscale
    frame = torchvision.transforms.functional.rgb_to_grayscale(frame)

    #downscale
    ## Generative AI
    frame = torch.nn.functional.interpolate(frame.unsqueeze(0), size=(ADJ_FRAME_HEIGHT, ADJ_FRAME_WIDTH), mode='bilinear').squeeze(0)
    ## end generative AI

    # normalize
    frame = frame / 255.0
    
    
    return(frame)

def initializeEnvironment(mode, randomLevel = True, rom = 'v3'):
    """Initialize environment in gymnasium. 
    Sets the list of acceptable actions.

    Inputs:
        mode - the set of available actions, either from JoypadSpace, or
               a custom set of actions provided as a list of lists of actions
        rom - the selected ROM
    Outputs:
        (env, actionSpace_init) - a tuple of the gymnasium environment and actionSpace

    Note that v0 offers a traditional view, corresponding 
    to 'super-mario-bros.nes' included with package
    with MD5 of: 673913a23cd612daf5ad32d4085e0760
    and is "Super Mario Bros. (E) (REVA) [!].nes SourceDB: GoodNES 3.23"
    as per: https://tasvideos.org/Games/1/Versions/List
    v3, in turn, is a simplified rectangular view
    this may have been generated by kautenja
    and does not appear in the TAS collections that I saw

    v0 is more visually appealing, but it seems plausible that v3
    would train faster.
    """
    if rom == 'v0':
        s = 'SuperMarioBros-v0'
    elif rom == 'v3':
        s = 'SuperMarioBros-v3'
    else:
        print("Error in ROM selection.")
        return(None)

    # as per documentation, SuperMarioBrosRandomStages-v0 will randomly select world, level combinations
    if randomLevel:
        s = s.split('-')
        s = 'RandomStages-'.join(s)
    
    env = gym_super_mario_bros.make(s)

    if mode == "simple":
        env = JoypadSpace(env, SIMPLE_MOVEMENT)
        actionSpace_init = SIMPLE_MOVEMENT
    elif mode == "complex":
        env = JoypadSpace(env, COMPLEX_MOVEMENT)
        actionSpace_init = COMPLEX_MOVEMENT
    elif mode == "rightOnly":
        env = JoypadSpace(env, RIGHT_ONLY)
        actionSpace_init = RIGHT_ONLY
    else:
        # provide a predefined list of string actions
        # eg., [['NOOP'], ['right', 'A'], ['right', 'B'], ['right', 'B', 'A']]
        env = JoypadSpace(env, mode)
        actionSpace_init = mode

    return( (env, actionSpace_init) )

def saveDiagnosticImage(folder, frameArray, step, action, x_pos, y_pos, rectangle):
    """ Save a snapshot with additional text info burned in.
    
    Inputs:
        folder - the folder in which to save images, NB., one folder per episode 
        frameArray - TODO, standardize, revise with prefillbuffer
        step, action - current step and action for burning in
        x_pos, y_pos - current x and y position for burning in
        
        """

    ## this method had meaningful assistance from generativeAI
    ## TODO: this should be revised for the workflow actually in use with additional documentation
    # Convert torch.Tensor to NumPy
    if isinstance(frameArray, torch.Tensor):
        frame = frameArray.detach().cpu()
        if frame.shape[0] == 1:  # Grayscale
            frame = frame.squeeze(0).numpy()  # (H, W)
        else:  # RGB
            frame = frame.permute(1, 2, 0).numpy()  # (H, W, C)
    else:
        frame = frameArray.copy()

    # Ensure dtype is uint8
    frame = np.clip(frame, 0, 255).astype(np.uint8)

    image = Image.fromarray(frame)
    
    draw = ImageDraw.Draw(image)
    ## fails in TensorFlow environment
    #font = ImageFont.truetype("arial.ttf", size = 20)
    text_annotation = ""
    text_annotation += str(f"step: {step:0>7}\naction: {action}\n")
    text_annotation += str(f"x: {x_pos:0>3}, y: {y_pos:0>3}\n")
 
    if image.mode == "L":
        backgroundColor = 0 # black
        fillColor = 255 # white for grayscale
    else:
        backgroundColor = (255, 255, 255) # white for RGB
        fillColor = (0,   0,   0  ) # black for RGB

    #              x0, y0, x1, y1
    draw.rectangle(rectangle, fill = backgroundColor)
    draw.text((0, 0), text_annotation, fill = fillColor)
 
    # use of padding in filename is helpful for passing 
    # in to Kdenlive as an Image Sequence for video review
    # in quick testing, .png files were smaller than .jpeg
    image.save(f"./{folder}/{step:0>7}.png")

def selectAction(phi):
    # select a_t = max_a Q∗(φ(st), a; θ)
    phi = phi.unsqueeze(0) # (c, w, h) -> (1, c, w, h)

    if random() < 0.01:
        return(randint(0, len(ACTION_SPACE_IN_USE) - 1)) #randint is inclusive of right   
    else:
        return(torch.argmax(Q(phi)).item()) # convert to an integer for indexing

## generated by generative AI
def compute_targets(minibatch, Q):
    phi_batch, action_batch, reward_batch, next_state_batch = minibatch

    # print(f"{action_batch.dtype =}")
    # print(f"{action_batch[0] = }")

    # Current Q-values for chosen actions
    q_values = Q(phi_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)

    # Next state Q-values
    next_q_values = Q(next_state_batch)
    max_next_q = next_q_values.max(dim=1)[0]

    # Targets
    y = reward_batch + GAMMA * max_next_q

    return q_values, y
## end generative AI


def runEpisode(seed = None,
               saveImage = False,
               saveImageFrequency = 30, # steps between a saved image, set to 1 for every frame
               printStatus = False,
               printFrequency = 1_000,
               debug = False,
               epoch = None
              ):

    """ Run a single episode until death. """

    cumulativeReward = 0
    maxEpisodeLength = 100_000 # in testing, this seems to be reasonable
    actions = np.empty(maxEpisodeLength, dtype = int)
    actions.fill(-1)
    
    # set up one folder for one episode
    if saveImage:
        timeFolder = datetime.datetime.now().strftime("%Y_%m_%d__%H_%M_%S")
        print(f"Saving images to : ./stateSequences/{timeFolder}")
        rawDir = f'./stateSequences/{timeFolder}/raw'
        preproDir = f'./stateSequences/{timeFolder}/preprocessed'
        os.mkdir(f'./stateSequences/{timeFolder}')
        os.mkdir(rawDir)
        os.mkdir(preproDir)
        #Q.saveModel(f"./stateSequences/{timeFolder}/model.pth")    

    

    state, info = env.reset(seed = seed) if SEED else env.reset()
    phi = preprocessFrame(state)
    if debug: print(f"{phi.shape = } --- should be [1, {ADJ_FRAME_WIDTH}, {ADJ_FRAME_HEIGHT}]")
    
    step = -1
    
    while True:
        step += 1

        # epsilon-greedy or otherwise select a_t = max_a Q^*(φ(st), a; θ) 
        action = selectAction(phi)
        actions[step] = action

        try:
            action_text = actionSpace[action]
        except IndexError:
            print(action, actionSpace)

        phi = preprocessFrame(state)
        state, reward, terminated, truncated, info = env.step(action)
        phiPrime = preprocessFrame(state)

        # store new transition
        D.storeTransition( (phi, action, reward, phiPrime) )
        cumulativeReward += reward

        train_frequency = 4 # train every 'x' frames
        if step % train_frequency == 0:
            # sample random minibatch of transitions (φ_j, a_j, r_j, φ_{j+1}) from D          
            minibatch = D.sample(BATCH_SIZE)
            # y = r_j + γ max_{a′} Q(φ_{j+1}, a′; θ) for non-terminal φ_{j+1} 
            # for the moment ignoring the terminal state
            q_values, y = compute_targets(minibatch, Q)
            loss = torch.nn.functional.mse_loss(q_values, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
  
        if terminated or truncated:
            finalUpdate = f"{step=:0>7}, {cumulativeReward=}, {loss = }, {info['coins']=}, {info['time']=}"
            if debug:
                print(f"{terminated=}\n{truncated=}")
                print(f"{info=}")

            # print terminal status
            if printStatus:
                with open('./results/log.txt', 'a') as o:
                    o.write(f"{finalUpdate}\n")
            break
            
        if saveImage:
            # print(state.shape)
            # print(phi.shape)
            
            # imgState = state.squeeze(0)
            # imgPhi = phi.squeeze(0)
            
            if step % saveImageFrequency == 0:
                rawRectangle = [0, 0, 70, 50]
                preproRectangle = [0, 0, 70, 50]
                saveDiagnosticImage(rawDir, state, step, action_text, info['x_pos'], info['y_pos'], rawRectangle)
                saveDiagnosticImage(preproDir, phi * 255.0, step, action_text, info['x_pos'], info['y_pos'], preproRectangle)

        # diagnostic info printing
        # print periodically, and as mario is timing out
        if printStatus and (step % printFrequency == 0 or step == 0):
            update = f"{step=:0>7}, {cumulativeReward=}, {loss = }, {info['coins']=}, {info['time']=}\n"
            with open('./results/log.txt', 'a') as o:
                o.write(update)
            #print(update)

    #pd.DataFrame({'actions' : actions}).to_csv(f'./stateSequences/{timeFolder}/actions.csv')
    #with open(f'./stateSequences/{timeFolder}/setup.txt/') as f:
    #    f.write(f"{cumulativeRewards = }")
    result = {}
    result['cumulativeReward'] = cumulativeReward
    #result['actions'] = actions # fixed length
    result['info'] = info
    # variable length to just the last action taken
    #lastActionIndex = np.argmax(actions == -1)
    #result['actualActions'] = actions[0: lastActionIndex]
    
    return(result)


def runEpisodes(numEpisodes):
    resultFolder = datetime.datetime.now().strftime("%Y_%m_%d__%H_%M_%S")
    resultFolder = f"./savedModels/{resultFolder}/"
    print(f"Saving models  to : {resultFolder}")
    os.mkdir(resultFolder)

    results = []

    emitConfig()

    with open('./results/perEpisodeRewards.csv', 'a', newline = '') as CSV_outfile:
        CSV_writer = csv.writer(CSV_outfile, delimiter = ',', quotechar = '|', quoting = csv.QUOTE_MINIMAL)
        CSV_writer.writerow(['episode', 'cumulativeReward', 'course', 'flag_get'])
        
        for i in range(numEpisodes):
            if i % 10 == 0 or i == 0:
                saveImage, printStatus = True, True
                with open('./results/log.txt', 'a') as o:
                    update = f"\n---\nEpisode: {i}\n"
                    o.write(update)   
            else:
                saveImage, printStatus = False, False
            result = runEpisode(printStatus = printStatus, saveImage = saveImage)
            results.append(result)
            ri = result['info']
            course = f"{ri['world']}-{ri['stage']}"
            CSV_writer.writerow([i, result['cumulativeReward'], course, ri['flag_get']])
            if i % 1_000 == 0 or i == 0:
                Q.saveModel(f"{resultFolder}{i}.pth")
            
    return(results)


def emitConfig():
    # from global
    vars = [FRAME_WIDTH, VTRIM, HTRIM, HTRIM_RIGHT, TRIM_FRAME_HEIGHT, TRIM_FRAME_WIDTH, ADJ_FRAME_HEIGHT, ADJ_FRAME_WIDTH, BUFFER_SIZE, SEED, ROM, BATCH_SIZE, GAMMA, LEARNING_RATE, BASIC_ACTION_SPACE, ACTION_SPACE_IN_USE]
    var_labels = ["FRAME_WIDTH", "VTRIM", "HTRIM", "HTRIM_RIGHT", "TRIM_FRAME_HEIGHT", "TRIM_FRAME_WIDTH", "ADJ_FRAME_HEIGHT", "ADJ_FRAME_WIDTH", "BUFFER_SIZE", "SEED", "ROM", "BATCH_SIZE", "GAMMA", "LEARNING_RATE", "BASIC_ACTION_SPACE", "ACTION_SPACE_IN_USE"]

    with open('./results/log.txt', 'a') as o:
        o.write("\n---\nStarting new set of episodes\n")
        o.write(f"{datetime.datetime.now()}\n")
        
        for var, label in zip(vars, var_labels):
            # separate label necessary {var = } will print 'var'
            o.write(f"{label} = {var}\n") 
            
# ------------------------------------
# constants
FRAME_HEIGHT = 240
FRAME_WIDTH = 256
VTRIM = 36
HTRIM = 36 # trim pixels off the left
HTRIM_RIGHT = 16 # trim pixels off the right
TRIM_FRAME_HEIGHT = FRAME_HEIGHT - VTRIM
TRIM_FRAME_WIDTH  = FRAME_WIDTH  - HTRIM - HTRIM_RIGHT
ADJ_FRAME_HEIGHT = 100 # downscaled from trimmed
ADJ_FRAME_WIDTH = 100  # 
BUFFER_SIZE = 1_000
SEED = None
ROM = 'v0'
BATCH_SIZE = 32
GAMMA = 0.99
LEARNING_RATE = 1e-4
BASIC_ACTION_SPACE = [['right'], ['left'], ['down'], ['up'], ['B'], ['A'], ['NOOP']]

ACTION_SPACE_IN_USE = [['right'], ['NOOP'], ['right', 'B'], ['right', 'A'], ['down']]
    

# ------------------------------------
# action spaces
# see action_space_and_TAS.ipynb
ALL_BUTTONS = ['right', 'left', 'down', 'up', 'start', 'select', 'B', 'A']
ALL_SINGLE_ACTIONS = ALL_BUTTONS.copy()
ALL_SINGLE_ACTIONS.append("NOOP")
COMPLETE_ACTION_SPACE = [ ["NOOP"] ] # note use of a list of lists 

for i in range(1, len(ALL_BUTTONS) + 1): # do not include the empty permutation
                                         # already included above
    for j in combinations(ALL_BUTTONS, i):
        COMPLETE_ACTION_SPACE.append(list(list(j)))


# ------------------------------------
# training

if __name__ == "__main__":

    if len(sys.argv) > 1:
        print("Received additional options:", sys.argv[1:])

    
    print(f"Original frame size (H x W): {FRAME_HEIGHT} x {FRAME_WIDTH}")
    print(f"Trimmed frame size (H x W): {TRIM_FRAME_HEIGHT} x {TRIM_FRAME_WIDTH}") 
    print(f"Downscaled frame size (H x W): {ADJ_FRAME_HEIGHT} x {ADJ_FRAME_WIDTH}")
    
    
    ## Initialize action-value function Q with random weights 
    print("\n---\nInitializing Deep Q Network")
    # Q = DQN(actionSpaceSize = len(BASIC_ACTION_SPACE),
    #         RGB = False,
    #         width  = ADJ_FRAME_WIDTH,
    #         height = ADJ_FRAME_HEIGHT)

    Q = DQN( input_shape = (1, ADJ_FRAME_WIDTH, ADJ_FRAME_HEIGHT),
             num_actions = len(ACTION_SPACE_IN_USE))
    
    print("Q-network info")
    print(f"{Q =}")
    print("A sample output's shape from the feature extractor: ", Q.featureExtractor(torch.zeros(1,1,ADJ_FRAME_WIDTH,ADJ_FRAME_HEIGHT)).shape)
    
    optimizer = optim.Adam(Q.parameters(), lr=LEARNING_RATE)
    
    print("\n---\nInitializing gymnasium environment")
    env, actionSpace = initializeEnvironment(ACTION_SPACE_IN_USE, rom = ROM)
    print(f"environment initialized with rom: {ROM}")
    
    ## Initialize replay memory D to capacity N
    print("\n---\nInitializing replay memory buffer")
    D = prefillBuffer(BUFFER_SIZE, env, actionSpace)
    print(f"Buffer filled with {BUFFER_SIZE} transitions.")