# figures/processing
import pandas as pd
import numpy as np

# logging
import csv

# environment
import gym_super_mario_bros 
import nes_py      
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY

# saving outputs
# from time import monotonic  
import os
#from pathlib import Path # https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories
import datetime
import matplotlib.pyplot as plt

from random import randint, random

import sys #argument handling

#from collections import deque
import torch
import torch.optim as optim
#import torchvision # grayscale

import helpers
import DQN
from replay_buffer import ReplayBuffer
from DQN import DQN
from itertools import combinations # combinations for a complete actionSpace


class DQNAgent():

    def __init__(self, 
                 Q,
                 replayBuffer,                                  
                 BATCH_SIZE = 32,
                 GAMMA = 0.99,
                 randomLevel = True, 
                 rom = 'v3',
                 stagesList = ['1-1'],
                 buttonList = [['NOOP']],
                 debug = True,
                ):

        self.Q = Q
        self.D = replayBuffer        
        self.BATCH_SIZE = BATCH_SIZE
        self.GAMMA = GAMMA

        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr = 1e-4)
        
        # environment initialization
        self.randomLevel = randomLevel
        self.rom = 'v0'
        self.buttonList = buttonList # name is descriptive and intentional, this doesn't become an actionSpace until it comes out from gymnasium

        # constants
        self.FRAME_HEIGHT = 240
        self.FRAME_WIDTH = 256
        self.VTRIM = 36
        self.HTRIM = 36 # trim pixels off the left
        self.HTRIM_RIGHT = 16 # trim pixels off the right
        self.TRIM_FRAME_HEIGHT = self.FRAME_HEIGHT - self.VTRIM
        self.TRIM_FRAME_WIDTH  = self.FRAME_WIDTH  - self.HTRIM - self.HTRIM_RIGHT
        self.ADJ_FRAME_HEIGHT = 100 # downscaled from trimmed
        self.ADJ_FRAME_WIDTH = 100  # 


        self.stagesList = stagesList
        self.env, self.actionSpace = helpers.initializeEnvironment(stagesList = self.stagesList, buttonList = self.buttonList, rom = self.rom)

    def __repr__(self):

        s = ''
        s += f"{self.stagesList = }\n"
        s += f"{self.GAMMA = }\n"

        return(s)
        
    def selectAction(self, phi):
        # select a_t = max_a Q∗(φ(st), a; θ)
        ##phi = phi.unsqueeze(0) # (c, w, h) -> (1, c, w, h)
    
        if random() < 0.01:
            return(randint(0, len(self.buttonList) - 1)) #randint is inclusive of right   
        else:
            # get the argmax output from Q-network
            # cast as integer to pass into gymnasium
            return(torch.argmax(self.Q(phi)).item())
    
    ## generated by generative AI, minor edits
    def compute_targets(self, minibatch):
        phi_batch, action_batch, reward_batch, next_state_batch = minibatch
    
        # print(f"{action_batch.dtype =}")
        # print(f"{action_batch[0] = }")
    
        # Current Q-values for chosen actions
        q_values = self.Q(phi_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)
    
        # Next state Q-values
        next_q_values = self.Q(next_state_batch)
        max_next_q = next_q_values.max(dim=1)[0]
    
        # Targets
        y = reward_batch + self.GAMMA * max_next_q
    
        return q_values, y
    ## end generative AI

    def runEpisode(self,
                   seed = None,
                   saveImage = False,
                   saveImageFrequency = 30, # steps between a saved image, set to 1 for every frame
                   printStatus = False,
                   printFrequency = 1_000, # steps between printing the status
                   debug = False,
                   epoch = None,
                  ):
    
        """ Run a single episode until death. """
    
        cumulativeReward = 0
        maxEpisodeLength = 20_000 # in testing, this seems to be reasonable, even accounts for degenerate puzzle course cases
        actions = np.empty(maxEpisodeLength, dtype = int)
        actions.fill(-1)
        
        # set up one folder for one episode
        if saveImage:
            timeFolder = datetime.datetime.now().strftime("%Y_%m_%d__%H_%M_%S")
            print(f"Saving images to : ./stateSequences/{timeFolder}")
            rawDir = f'./stateSequences/{timeFolder}/raw'
            preproDir = f'./stateSequences/{timeFolder}/preprocessed'
            os.mkdir(f'./stateSequences/{timeFolder}')
            os.mkdir(rawDir)
            os.mkdir(preproDir)    
        
        state, info = self.env.reset(seed = seed) if seed else self.env.reset()
        if state is None:
            print("state is none!?")
        phi = helpers.preprocessFrame(state)
        if phi is None:
            print("phi is none!?")
        phiT = helpers.tensorify(phi)
        if debug: print(f"{phiT.shape = } --- should be [1, {self.ADJ_FRAME_WIDTH}, {self.ADJ_FRAME_HEIGHT}]")
        
        step = -1
        
        while True:
            step += 1
    
            # epsilon-greedy or otherwise select a_t = max_a Q^*(φ(st), a; θ) 
            action = self.selectAction(phiT)
            actions[step] = action

            # TODO: remove
            try:
                action_text = self.actionSpace[action]
            except IndexError:
                print(action, self.actionSpace)
    
            state, reward, terminated, truncated, info = self.env.step(action)
            phiPrime = helpers.preprocessFrame(state)
            phiPrimeT = helpers.tensorify(phiPrime)
    
            # store new transition
            self.D.storeTransition( (phiT, action, reward, phiPrimeT) )
            cumulativeReward += reward

            phiT = phiPrimeT
    
            train_frequency = 4 # train every 'x' frames
            if step % train_frequency == 0:
                # sample random minibatch of transitions (φ_j, a_j, r_j, φ_{j+1}) from D          
                minibatch = self.D.sample(self.BATCH_SIZE)
                # y = r_j + γ max_{a′} Q(φ_{j+1}, a′; θ) for non-terminal φ_{j+1} 
                # for the moment ignoring the terminal state
                q_values, y = self.compute_targets(minibatch)
                loss = torch.nn.functional.mse_loss(q_values, y)
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
      
            if terminated or truncated:
                finalUpdate = f"{step=:0>7}, {cumulativeReward=}, {loss = }, {info['coins']=}, {info['time']=}"
                if debug:
                    print(f"{terminated=}\n{truncated=}")
                    print(f"{info=}")
    
                # print terminal status
                if printStatus:
                    with open('./results/log.txt', 'a') as o:
                        o.write(f"{finalUpdate}\n")
                breaqk
                
            if saveImage:
                # print(state.shape)
                # print(phi.shape)
                
                # imgState = state.squeeze(0)
                # imgPhi = phi.squeeze(0)
                
                if step % saveImageFrequency == 0:
                    rawRectangle = [0, 0, 70, 50]
                    preproRectangle = [0, 0, 70, 50]
                    helpers.saveDiagnosticImage(rawDir, state, step, action_text, info['x_pos'], info['y_pos'], rawRectangle)
                    helpers.saveDiagnosticImage(preproDir, phi * 255.0, step, action_text, info['x_pos'], info['y_pos'], preproRectangle)
    
            # diagnostic info printing
            # print periodically, and as mario is timing out
            if printStatus and (step % printFrequency == 0 or step == 0):
                update = f"{step=:0>7}, {cumulativeReward=}, {loss = }, {info['coins']=}, {info['time']=}\n"
                with open('./results/log.txt', 'a') as o:
                    o.write(update)
                #print(update)
    
        #pd.DataFrame({'actions' : actions}).to_csv(f'./stateSequences/{timeFolder}/actions.csv')
        #with open(f'./stateSequences/{timeFolder}/setup.txt/') as f:
        #    f.write(f"{cumulativeRewards = }")
        result = {}
        result['cumulativeReward'] = cumulativeReward
        #result['actions'] = actions # fixed length
        result['info'] = info
        # variable length to just the last action taken
        #lastActionIndex = np.argmax(actions == -1)
        #result['actualActions'] = actions[0: lastActionIndex]
        
        return(result)
    
    
    def runEpisodes(self, numEpisodes):
        resultFolder = datetime.datetime.now().strftime("%Y_%m_%d__%H_%M_%S")
        resultFolder = f"./savedModels/{resultFolder}/"
        print(f"Saving models  to : {resultFolder}")
        os.mkdir(resultFolder)
    
        results = []
    
        #emitConfig()
    
        with open('./results/perEpisodeRewards.csv', 'a', newline = '') as CSV_outfile:
            CSV_writer = csv.writer(CSV_outfile, delimiter = ',', quotechar = '|', quoting = csv.QUOTE_MINIMAL)
            CSV_writer.writerow(['episode', 'cumulativeReward', 'course', 'flag_get'])
            
            for i in range(numEpisodes):
                if i % 50 == 0 or i == 0:
                    saveImage, printStatus = True, True
                    with open('./results/log.txt', 'a') as o:
                        update = f"\n---\nEpisode: {i}\n"
                        o.write(update)   
                else:
                    saveImage, printStatus = False, False
                result = self.runEpisode(printStatus = printStatus, saveImage = saveImage)
                results.append(result)
                ri = result['info']
                course = f"{ri['world']}-{ri['stage']}"
                CSV_writer.writerow([i, result['cumulativeReward'], course, ri['flag_get']])
                if i % 1_000 == 0 or i == 0:
                    self.Q.saveModel(f"{resultFolder}{i}.pth")
                
        return(results)